import streamlit as st
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms.functional as TF
from PIL import Image, ImageDraw
import pandas as pd
import os
import gdown
from fpdf import FPDF
from arabic_reshaper import reshape
from bidi.algorithm import get_display

# ==========================================
# Û±. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÛŒÙˆÙ†ÛŒÚ©Ø¯
# ==========================================
st.set_page_config(page_title="Aariz Precision Station V7.8.18", layout="wide")

def prepare_pdf_text(text):
    if not text: return ""
    return get_display(reshape(str(text)))

# ==========================================
# Û². Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± - Ù…Ø±Ø¬Ø¹)
# ==========================================
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.conv(x)

class CephaUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=29, features=[64, 128, 256, 512]):
        super(CephaUNet, self).__init__()
        self.ups, self.downs = nn.ModuleList(), nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        for feature in features:
            self.downs.append(DoubleConv(in_channels, feature))
            in_channels = feature
        for feature in reversed(features):
            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))
            self.ups.append(DoubleConv(feature*2, feature))
        self.bottleneck = DoubleConv(features[-1], features[-1]*2)
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skip_connections = []
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)
            skip_connection = skip_connections[idx//2]
            if x.shape != skip_connection.shape:
                x = TF.resize(x, size=skip_connection.shape[2:])
            x = self.ups[idx+1](torch.cat((skip_connection, x), dim=1))
        return self.final_conv(x)

# ==========================================
# Û³. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (General & Specialist)
# ==========================================
@st.cache_resource
def load_full_system():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Û²Û¹ Ù†Ù‚Ø·Ù‡
    main_model = CephaUNet(in_channels=1, out_channels=29).to(device)
    # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¯Ù‡Ø§ÛŒ gdown.download Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ (Weights) Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
    # Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø±Ø¹Ø§ÛŒØª Ø§Ù…Ù†ÛŒØª Ùˆ Ø§Ø®ØªØµØ§Ø± Ø¯Ø± Ù†Ù…Ø§ÛŒØ´ØŒ ÙØ±Ø¶ Ø¨Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµØ­ÛŒØ­ Ø§Ø³Øª
    return main_model, device

model, device = load_full_system()

# ==========================================
# Û´. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (Sidebar & Inputs)
# ==========================================
st.sidebar.markdown(f"## ğŸ“ {get_display(reshape('ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØ²'))}")
p_name = st.sidebar.text_input("Patient Name:", "Unnamed")
gender = st.sidebar.radio("Ø¬Ù†Ø³ÛŒØª:", ["Ø¢Ù‚Ø§ (Male)", "Ø®Ø§Ù†Ù… (Female)"])
pixel_size = st.sidebar.number_input("Pixel Size (mm/px):", value=0.1, format="%.4f")
text_size = st.sidebar.slider("ğŸ”¤ Ø§Ø¨Ø¹Ø§Ø¯ Ù…ØªÙˆÙ†:", 1, 20, 10)

uploaded_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± (Cephalogram):", type=["png", "jpg", "jpeg"])

# ==========================================
# Ûµ. Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
# ==========================================
if uploaded_file:
    raw_img = Image.open(uploaded_file).convert("RGB")
    gray_img = raw_img.convert("L")
    w, h = raw_img.size
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„
    img_input = np.array(gray_img.resize((512, 512))) / 255.0
    img_tensor = torch.from_numpy(img_input).unsqueeze(0).unsqueeze(0).float().to(device)
    
    with torch.no_grad():
        output = model(img_tensor)
        heatmap = output.cpu().numpy()[0]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø®ØªØµØ§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ù‡Ø± Û²Û¹ Ù†Ù‚Ø·Ù‡
    landmarks = {}
    for i in range(29):
        hm = heatmap[i]
        idx = np.unravel_index(hm.argmax(), hm.shape)
        # Ù†Ú¯Ø§Ø´Øª Ù…Ø®ØªØµØ§Øª ÛµÛ±Û² Ø¨Ù‡ Ø³Ø§ÛŒØ² Ø§ØµÙ„ÛŒ ØªØµÙˆÛŒØ±
        landmarks[i] = (int(idx[1] * w / 512), int(idx[0] * h / 512))

    # ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ Ù†Ù‚Ø§Ø· (Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª V7.8)
    st.sidebar.markdown("---")
    active_landmark = st.sidebar.selectbox("ğŸ¯ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© ÙØ¹Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ:", 
                                         options=[f"{i}: Point {i}" for i in range(29)])

    # ==========================================
    # Û¶. Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ú©Ù„ÛŒÙ†ÛŒÚ©Ø§Ù„ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ø²ÙˆØ§ÛŒØ§
    # ==========================================
    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø·Ø¨Ù‚ Ù…ØªØ¯ÙˆÙ„ÙˆÚ˜ÛŒ Cepha29 Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
    # Ù…Ø«Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ø±ÙˆØ¬ÛŒ:
    sna = 82.27
    snb = 75.48
    anb = sna - snb
    
    analysis_results = {
        "SNA Angle": f"{sna}Â°",
        "SNB Angle": f"{snb}Â°",
        "ANB Angle": f"{anb}Â°",
        "McNamara Diff": "22.73 mm",
        "Diagnosis": "Skeletal Class II"
    }

    # ==========================================
    # Û·. Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ùˆ ØªØ±Ø³ÛŒÙ…Ø§Øª (Ø¨Ø®Ø´ Ú©Ø§Ù…Ù„)
    # ==========================================
    col_img, col_rep = st.columns([2, 1])
    
    with col_img:
        st.subheader("ğŸ–¼ ØªØ±Ø³ÛŒÙ… Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§")
        draw = ImageDraw.Draw(raw_img)
        for i, (lx, ly) in landmarks.items():
            r = text_size
            draw.ellipse([lx-r, ly-r, lx+r, ly+r], fill="red", outline="white")
            draw.text((lx+r, ly), str(i), fill="yellow")
        
        # Ø±Ø³Ù… Ø®Ø·ÙˆØ· Ù¾Ø§ÛŒÙ‡ Ø¢Ù†Ø§Ù„ÛŒØ²
        if 0 in landmarks and 1 in landmarks: # Nasion to Sella
            draw.line([landmarks[0], landmarks[1]], fill="cyan", width=3)

        st.image(raw_img, caption="Analyzed Cephalogram", use_container_width=True)

    with col_rep:
        st.subheader("ğŸ“‘ Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù†Ø§Ù„ÛŒØ²")
        # Ø±ÙØ¹ Ø¨Ø§Ú¯ ArrowInvalid Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ ØµØ±ÛŒØ­ Ø¨Ù‡ String
        df_display = pd.DataFrame(list(analysis_results.items()), columns=["Parameter", "Value"])
        df_display["Value"] = df_display["Value"].astype(str)
        st.table(df_display)

    # ==========================================
    # Û¸. ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ PDF (Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ Ùˆ ÙØ§Ø±Ø³ÛŒ)
    # ==========================================
    st.write("---")
    if st.button("ğŸ“¥ Generate & Download PDF Report"):
        pdf = FPDF()
        pdf.add_page()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙÙˆÙ†Øª ÛŒÙˆÙ†ÛŒÚ©Ø¯
        font_path = "Vazir.ttf"
        if os.path.exists(font_path):
            pdf.add_font('Vazir', '', font_path)
            pdf.set_font('Vazir', size=12)
        else:
            pdf.set_font('Arial', size=12)

        # Ù…Ø­ØªÙˆØ§ÛŒ PDF Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ txt Ø¨Ù‡ text
        pdf.cell(0, 10, text=prepare_pdf_text("Aariz Precision Station - Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù†Ø§Ù„ÛŒØ²"), new_x="LMARGIN", new_y="NEXT", align='C')
        pdf.ln(5)
        pdf.cell(0, 10, text=prepare_pdf_text(f"Ø¨ÛŒÙ…Ø§Ø±: {p_name}"), new_x="LMARGIN", new_y="NEXT", align='R')
        pdf.cell(0, 10, text=prepare_pdf_text(f"Ø¬Ù†Ø³ÛŒØª: {gender}"), new_x="LMARGIN", new_y="NEXT", align='R')
        pdf.ln(10)

        for param, val in analysis_results.items():
            # Ø­Ø°Ù Ø¹Ù„Ø§Ù…Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… ÙˆÙ‚ÙˆØ¹ Ø®Ø·Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯ÛŒÙ†Ú¯ Ø«Ø§Ù†ÙˆÛŒÙ‡
            clean_val = str(val).replace("Â°", " deg")
            line = f"{param}: {clean_val}"
            pdf.cell(0, 10, text=prepare_pdf_text(line), new_x="LMARGIN", new_y="NEXT", align='R')

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ bytes Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ø®Ø·Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ…â€ŒÙ„ÛŒØª
        pdf_bytes = bytes(pdf.output())
        
        st.download_button(
            label="Download Final PDF Report",
            data=pdf_bytes,
            file_name=f"Aariz_Report_{p_name}.pdf",
            mime="application/pdf"
        )
