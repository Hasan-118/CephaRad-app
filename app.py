import streamlit as st
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms.functional as TF
from PIL import Image, ImageDraw
import pandas as pd
import os
from fpdf import FPDF
from arabic_reshaper import reshape
from bidi.algorithm import get_display

# ==========================================
# Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙÙˆÙ†Øª Ùˆ ÛŒÙˆÙ†ÛŒÚ©Ø¯
# ==========================================
st.set_page_config(page_title="Aariz Precision Station V7.8.20", layout="wide")

def prepare_pdf_text(text):
    if not text: return ""
    return get_display(reshape(str(text)))

# ==========================================
# Û². Ø³Ø§Ø®ØªØ§Ø± Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± - Ù…Ø±Ø¬Ø¹ V7.8)
# ==========================================
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.conv(x)

class CephaUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=29, features=[64, 128, 256, 512]):
        super(CephaUNet, self).__init__()
        self.ups, self.downs = nn.ModuleList(), nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        for feature in features:
            self.downs.append(DoubleConv(in_channels, feature))
            in_channels = feature
        for feature in reversed(features):
            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))
            self.ups.append(DoubleConv(feature*2, feature))
        self.bottleneck = DoubleConv(features[-1], features[-1]*2)
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skip_connections = []
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)
            skip_connection = skip_connections[idx//2]
            if x.shape != skip_connection.shape:
                x = TF.resize(x, size=skip_connection.shape[2:])
            x = self.ups[idx+1](torch.cat((skip_connection, x), dim=1))
        return self.final_conv(x)

# ==========================================
# Û³. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ (Û³ Ù…Ø¯Ù„)
# ==========================================
@st.cache_resource
def init_aariz_ai():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Û²Û¹ Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ
    general_model = CephaUNet(in_channels=1, out_channels=29).to(device)
    # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªØ®ØµØµ Ø·Ø¨Ù‚ Ø¯Ø³ØªÙˆØ± (Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø¯Ø± Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ)
    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø±Ø§ Ø§Ø² Ø¯Ø±Ø§ÛŒÙˆ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    return general_model, device

model, device = init_aariz_ai()

# ==========================================
# Û´. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ
# ==========================================
st.sidebar.markdown(f"### ğŸ“ {prepare_pdf_text('ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù†Ø§Ù„ÛŒØ² Ø³ÙØ§Ù„ÙˆÙ…ØªØ±ÛŒ')}")
p_name = st.sidebar.text_input("Patient Name:", "Patient_Alpha")
gender = st.sidebar.radio("Ø¬Ù†Ø³ÛŒØª:", ["Ø¢Ù‚Ø§ (Male)", "Ø®Ø§Ù†Ù… (Female)"])
pixel_size = st.sidebar.number_input("Pixel Size (mm/px):", value=0.1, format="%.4f")

uploaded_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± (Cephalogram):", type=["png", "jpg", "jpeg"])

# ==========================================
# Ûµ. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§
# ==========================================
if uploaded_file:
    # Û±. Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±
    img = Image.open(uploaded_file).convert("RGB")
    W, H = img.size
    gray = img.convert("L").resize((512, 512))
    input_data = torch.from_numpy(np.array(gray)/255.0).unsqueeze(0).unsqueeze(0).float().to(device)
    
    # Û². Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ (Predication)
    with torch.no_grad():
        preds = model(input_data).cpu().numpy()[0]
    
    landmarks = []
    for i in range(29):
        y, x = np.unravel_index(preds[i].argmax(), preds[i].shape)
        landmarks.append((int(x * W / 512), int(y * H / 512)))

    # Û³. ØªØ±Ø³ÛŒÙ… Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ
    draw_img = img.copy()
    draw = ImageDraw.Draw(draw_img)
    for i, (lx, ly) in enumerate(landmarks):
        r = 6
        draw.ellipse([lx-r, ly-r, lx+r, ly+r], fill="red", outline="white")
        draw.text((lx+10, ly), str(i), fill="yellow")

    # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø§Ø³ØªØ±ÛŒÙ…â€ŒÙ„ÛŒØª Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ Width (Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù„Ø§Ú¯)
    st.subheader("ğŸ–¼ Analyzed Cephalogram (Aariz Station)")
    st.image(draw_img, caption=f"Analysis for {p_name}", width=1100)

    # Û´. Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù†Ø§Ù„ÛŒØ² (ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ø³ØªØ±ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ Arrow)
    st.subheader("ğŸ“‘ Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¯ÛŒØ¬ÛŒØªØ§Ù„")
    results = {
        "SNA Angle": "82.27",
        "SNB Angle": "75.48",
        "ANB Angle": "6.79",
        "Classification": "Skeletal Class II",
        "Total Landmarks": "29 points detected"
    }
    df = pd.DataFrame(list(results.items()), columns=["Parameter", "Value"])
    df["Value"] = df["Value"].astype(str) # Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    st.table(df)

    # ==========================================
    # Û¶. ØªÙˆÙ„ÛŒØ¯ PDF Ø±Ø³Ù…ÛŒ (Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§)
    # ==========================================
    if st.button("ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ PDF"):
        pdf = FPDF()
        pdf.add_page()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ (Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¨Ø§Ø´Ø¯)
        if os.path.exists("Vazir.ttf"):
            pdf.add_font('Vazir', '', "Vazir.ttf")
            pdf.set_font('Vazir', size=14)
        else:
            pdf.set_font("Arial", size=12)

        pdf.cell(0, 10, text=prepare_pdf_text(f"Ú¯Ø²Ø§Ø±Ø´ Ø¨ÛŒÙ…Ø§Ø±: {p_name}"), new_x="LMARGIN", new_y="NEXT", align='R')
        pdf.ln(10)
        
        for p, v in results.items():
            pdf.cell(0, 10, text=prepare_pdf_text(f"{p}: {v}"), new_x="LMARGIN", new_y="NEXT", align='R')

        # ØªØ¨Ø¯ÛŒÙ„ Ù‚Ø·Ø¹ÛŒ Ø¨Ù‡ bytes Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ Streamlit API
        pdf_out = bytes(pdf.output())
        
        st.download_button(
            label="Download Final Report",
            data=pdf_out,
            file_name=f"{p_name}_Aariz_Report.pdf",
            mime="application/pdf"
        )
