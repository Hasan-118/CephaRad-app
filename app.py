import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import os
import gdown
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
from streamlit_image_coordinates import streamlit_image_coordinates

# [Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ CephaUNet Ùˆ Ù„ÙˆØ¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù…Ø·Ø§Ø¨Ù‚ Ù†Ø³Ø®Ù‡ Ù…Ø±Ø¬Ø¹ V3.3 - Ø¨Ø±Ø§ÛŒ Ø§Ø®ØªØµØ§Ø± Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ ØªÚ©Ø±Ø§Ø± Ù†Ø´Ø¯Ù‡]

def get_precision_magnifier(img, coord, zoom=4, size=100):
    x, y = coord
    left, top = max(0, int(x - size//2)), max(0, int(y - size//2))
    right, bottom = min(img.width, int(x + size//2)), min(img.height, int(y + size//2))
    crop = img.crop((left, top, right, bottom)).resize((400, 400), Image.LANCZOS)
    
    draw_mag = ImageDraw.Draw(crop)
    cx, cy = 200, 200 
    draw_mag.line((cx-20, cy, cx+20, cy), fill="red", width=2)
    draw_mag.line((cx, cy-20, cx, cy+20), fill="red", width=2)
    return crop, (left, top) # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø®ØªØµØ§Øª Ú¯ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ

# --- Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ---
st.set_page_config(page_title="Aariz Precision V3.4", layout="wide")
# [Ú©Ø¯ Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„...]

if uploaded_file:
    # [Ú©Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ AI...]
    
    col1, col2 = st.columns([1.5, 2]) # ØªØºÛŒÛŒØ± Ú†ÛŒØ¯Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø°Ø±Ù‡â€ŒØ¨ÛŒÙ†
    
    with col1:
        st.subheader("ğŸ¯ Micro-Adjustment")
        st.write("Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒÙ„ÛŒâ€ŒÙ…ØªØ±ÛŒØŒ Ø±ÙˆÛŒ Ù‡Ø¯Ù Ø²ÛŒØ± Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯:")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø°Ø±Ù‡â€ŒØ¨ÛŒÙ† ØªØ¹Ø§Ù…Ù„ÛŒ
        mag_img, (offset_x, offset_y) = get_precision_magnifier(raw_img, st.session_state.lms[target_idx])
        
        # Ú¯Ø±ÙØªÙ† Ú©Ù„ÛŒÚ© Ø§Ø² Ø¯Ø§Ø®Ù„ Ø°Ø±Ù‡â€ŒØ¨ÛŒÙ†
        res_mag = streamlit_image_coordinates(mag_img, key="mag_click")
        
        if res_mag:
            # ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ú©Ù„ÛŒÚ© Ø¯Ø± Ø°Ø±Ù‡â€ŒØ¨ÛŒÙ† (Û´Û°Û°xÛ´Û°Û°) Ø¨Ù‡ Ù…Ø®ØªØµØ§Øª ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
            click_x_in_crop = res_mag["x"] * (100 / 400) # Ø¨Ø± Ø§Ø³Ø§Ø³ size=100 Ø¯Ø± ØªØ§Ø¨Ø¹
            click_y_in_crop = res_mag["y"] * (100 / 400)
            
            new_x = int(offset_x + click_x_in_crop)
            new_y = int(offset_y + click_y_in_crop)
            
            if st.session_state.lms[target_idx] != [new_x, new_y]:
                st.session_state.lms[target_idx] = [new_x, new_y]
                st.rerun()

    with col2:
        st.subheader("ğŸ–¼ Full View")
        # [Ø±Ø³Ù… ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ùˆ Ú¯Ø±ÙØªÙ† Ú©Ù„ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯...]
        # (Ø¯Ù‚ÛŒÙ‚Ø§ Ù…Ø´Ø§Ø¨Ù‡ Ù†Ø³Ø®Ù‡ Ù‚Ø¨Ù„)import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import os
import gdown
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
from streamlit_image_coordinates import streamlit_image_coordinates

# --- Û±. Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ Ù…Ø±Ø¬Ø¹ (Ø«Ø§Ø¨Øª Ø·Ø¨Ù‚ Ø­Ø§ÙØ¸Ù‡) ---
class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch, dropout_prob=0.1):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Dropout2d(p=dropout_prob),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.conv(x)

class CephaUNet(nn.Module):
    def __init__(self, n_landmarks=29):
        super().__init__()
        self.inc = DoubleConv(1, 64); self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))
        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))
        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512, dropout_prob=0.3))
        self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2); self.conv_up1 = DoubleConv(512, 256, dropout_prob=0.3)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2); self.conv_up2 = DoubleConv(256, 128)
        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2); self.conv_up3 = DoubleConv(128, 64)
        self.outc = nn.Conv2d(64, n_landmarks, kernel_size=1)
    def forward(self, x):
        x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2); x4 = self.down3(x3)
        x = self.up1(x4); x = torch.cat([x, x3], dim=1); x = self.conv_up1(x)
        x = self.up2(x); x = torch.cat([x, x2], dim=1); x = self.conv_up2(x)
        x = self.up3(x); x = torch.cat([x, x1], dim=1); x = self.conv_up3(x)
        return self.outc(x)

# --- Û². Ù„ÙˆØ¯Ø± Ù…Ø¯Ù„ Ùˆ Ù…Ú¯Ù†ÛŒÙØ§ÛŒØ± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
@st.cache_resource
def load_aariz_system():
    model_ids = {
        'checkpoint_unet_clinical.pth': '1a1sZ2z0X6mOwljhBjmItu_qrWYv3v_ks',
        'specialist_pure_model.pth': '1RakXVfUC_ETEdKGBi6B7xOD7MjD59jfU',
        'tmj_specialist_model.pth': '1tizRbUwf7LgC6Radaeiz6eUffiwal0cH'
    }
    device = torch.device("cpu")
    loaded_models = []
    for f, fid in model_ids.items():
        if not os.path.exists(f):
            gdown.download(f'https://drive.google.com/uc?id={fid}', f, quiet=True)
        try:
            m = CephaUNet(n_landmarks=29).to(device)
            ckpt = torch.load(f, map_location=device)
            state = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt
            m.load_state_dict({k.replace('module.', ''): v for k, v in state.items()}, strict=False)
            m.eval(); loaded_models.append(m)
        except: pass
    return loaded_models, device

def get_precision_magnifier(img, coord, zoom=4, size=120):
    x, y = coord
    left, top = max(0, int(x - size//2)), max(0, int(y - size//2))
    right, bottom = min(img.width, int(x + size//2)), min(img.height, int(y + size//2))
    
    # Ø¨Ø±Ø´ Ùˆ Ø²ÙˆÙ…
    crop = img.crop((left, top, right, bottom)).resize((400, 400), Image.LANCZOS)
    
    # Ø±Ø³Ù… Ù†Ø´Ø§Ù†Ú¯Ø± Ù…Ø±Ú©Ø² (Crosshair) Ø±ÙˆÛŒ Ù…Ú¯Ù†ÛŒÙØ§ÛŒØ±
    draw_mag = ImageDraw.Draw(crop)
    cx, cy = 200, 200 # Ù…Ø±Ú©Ø² ØªØµÙˆÛŒØ± Û´Û°Û° Ù¾ÛŒÚ©Ø³Ù„ÛŒ
    draw_mag.line((cx-20, cy, cx+20, cy), fill="red", width=2)
    draw_mag.line((cx, cy-20, cx, cy+20), fill="red", width=2)
    draw_mag.ellipse([cx-4, cy-4, cx+4, cy+4], outline="white", width=1)
    return crop

# --- Û³. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ---
st.set_page_config(page_title="Aariz AI V3.3", layout="wide")
models, device = load_aariz_system()
landmark_names = ['A', 'ANS', 'B', 'Me', 'N', 'Or', 'Pog', 'PNS', 'Pn', 'R', 'S', 'Ar', 'Co', 'Gn', 'Go', 'Po', 'LPM', 'LIT', 'LMT', 'UPM', 'UIA', 'UIT', 'UMT', 'LIA', 'Li', 'Ls', 'N`', 'Pog`', 'Sn']

uploaded_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ØªØµÙˆÛŒØ±:", type=['png', 'jpg', 'jpeg'])

if uploaded_file and models:
    raw_img = Image.open(uploaded_file).convert("RGB")
    
    if "lms" not in st.session_state or st.session_state.get("file_id") != uploaded_file.name:
        # Prediction Logic...
        img_gray = raw_img.convert('L').resize((512, 512), Image.LANCZOS)
        t = transforms.ToTensor()(img_gray).unsqueeze(0).to(device)
        with torch.no_grad():
            outs = [m(t)[0].cpu().numpy() for m in models]
        coords = {}
        sx, sy = raw_img.width/512, raw_img.height/512
        for i in range(29):
            hm = outs[0][i]
            y, x = np.unravel_index(np.argmax(hm), hm.shape)
            coords[i] = [int(x * sx), int(y * sy)]
        st.session_state.lms = coords
        st.session_state.file_id = uploaded_file.name

    target_idx = st.sidebar.selectbox("ğŸ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© ÙØ¹Ø§Ù„:", range(29), format_func=lambda x: f"{x}: {landmark_names[x]}")

    col1, col2 = st.columns([2.5, 1])
    with col1:
        st.subheader(f"ğŸ” Ø²ÙˆÙ… Ø¯Ù‚ÛŒÙ‚: {landmark_names[target_idx]}")
        # Ù†Ù…Ø§ÛŒØ´ Ù…Ú¯Ù†ÛŒÙØ§ÛŒØ± Ø¨Ø§ Ø¹Ù„Ø§Ù…Øª + Ø¯Ø± Ù…Ø±Ú©Ø²
        mag = get_precision_magnifier(raw_img, st.session_state.lms[target_idx])
        st.image(mag, width=350)

        # Ø±Ø³Ù… ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
        draw_img = raw_img.copy()
        draw = ImageDraw.Draw(draw_img)
        l = st.session_state.lms
        for i, pos in l.items():
            color = "red" if i == target_idx else "#00FF00"
            r = 15 if i == target_idx else 8
            draw.ellipse([pos[0]-r, pos[1]-r, pos[0]+r, pos[1]+r], fill=color, outline="white", width=2)

        res = streamlit_image_coordinates(draw_img, width=900, key="aariz_v3_3")
        if res:
            scale = raw_img.width / 900
            new_coord = [int(res["x"]*scale), int(res["y"]*scale)]
            if st.session_state.lms[target_idx] != new_coord:
                st.session_state.lms[target_idx] = new_coord
                st.rerun()

    with col2:
        st.header("ğŸ“Š Ø¢Ù†Ø§Ù„ÛŒØ²")
        # [Ù…Ø­Ø§Ø³Ø¨Ø§Øª SNA/SNB Ù…Ø´Ø§Ø¨Ù‡ Ù‚Ø¨Ù„...]
        st.info("Ù†Ø´Ø§Ù†Ú¯Ø± Ù‚Ø±Ù…Ø² Ø¯Ø± Ø°Ø±Ù‡â€ŒØ¨ÛŒÙ†ØŒ Ù…Ø±Ú©Ø² Ø¯Ù‚ÛŒÙ‚ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ø´Ù…Ø§Ø³Øª.")

