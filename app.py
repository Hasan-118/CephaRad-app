import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
from streamlit_image_coordinates import streamlit_image_coordinates
from fpdf import FPDF
import arabic_reshaper
from bidi.algorithm import get_display
import requests
import io
import os

# --- Configuration & Styling ---
st.set_page_config(page_title="Aariz Precision Station V7.8.17", layout="wide")
st.markdown("""
    <style>
    .report-text { font-family: 'Tahoma'; direction: rtl; text-align: right; }
    .stButton>button { width: 100%; border-radius: 5px; height: 3em; background-color: #007bff; color: white; }
    </style>
    """, unsafe_allow_html=True)

# --- Model Architectures (Reference Standard) ---
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.conv(x)

class CephaUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=29):
        super().__init__()
        self.ups = nn.ModuleList()
        self.downs = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        features = [64, 128, 256, 512]
        for feature in features:
            self.downs.append(DoubleConv(in_channels, feature))
            in_channels = feature
        for feature in reversed(features):
            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))
            self.ups.append(DoubleConv(feature*2, feature))
        self.bottleneck = DoubleConv(features[-1], features[-1]*2)
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skip_connections = []
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        x = self.bottleneck(x)
        skip_connections = skip_connections[::-1]
        for i in range(0, len(self.ups), 2):
            x = self.ups[i](x)
            skip_connection = skip_connections[i//2]
            concat_skip = torch.cat((skip_connection, x), dim=1)
            x = self.ups[i+1](concat_skip)
        return self.final_conv(x)

# --- Utilities & Model Loading ---
@st.cache_resource
def load_models():
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªØ®ØµØµâ€ŒÙ‡Ø§ (Ø·Ø¨Ù‚ Ø¯Ø±Ø®ÙˆØ§Ø³Øª: Û³ Ù…Ø¯Ù„ Ù‡Ù…Ø²Ù…Ø§Ù†)
    # Ø§ÛŒÙ† Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ø¯Ø± Google Drive ÛŒØ§ Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´ÙˆÙ†Ø¯
    base_model = CephaUNet(out_channels=29)
    expert_1 = CephaUNet(out_channels=29)
    expert_2 = CephaUNet(out_channels=29)
    # base_model.load_state_dict(torch.load("path_to_model", map_location="cpu"))
    return base_model, expert_1, expert_2

def get_predictions(image, models):
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ (Û²Û¹ Ù†Ù‚Ø·Ù‡)
    transform = transforms.Compose([
        transforms.Grayscale(),
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
    ])
    img_tensor = transform(image).unsqueeze(0)
    with torch.no_grad():
        output = models[0](img_tensor) # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ
        preds = output.squeeze(0).cpu().numpy()
    
    landmarks = []
    for i in range(preds.shape[0]):
        y, x = np.unravel_index(preds[i].argmax(), preds[i].shape)
        landmarks.append([x * (image.width / 512), y * (image.height / 512)])
    return np.array(landmarks)

# --- UI Layout ---
st.title("ğŸ“ Aariz Precision Station V7.8.17")

with st.sidebar:
    st.header("ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨ÛŒÙ…Ø§Ø±")
    gender = st.selectbox("Ø¬Ù†Ø³ÛŒØª Ø¨ÛŒÙ…Ø§Ø±:", ["Ø¢Ù‚Ø§ (Male)", "Ø®Ø§Ù†Ù… (Female)"])
    pixel_size = st.number_input("Pixel Size (mm/px):", value=0.1, format="%.4f")
    
    st.header("Ù…Ù‚ÛŒØ§Ø³ Ù†Ø§Ù… Ù„Ù†Ø¯Ù…Ø§Ø±Ú©")
    label_scale = st.slider("Ø³Ø§ÛŒØ² ÙÙˆÙ†Øª:", 1, 20, 10)

# --- File Upload & Processing ---
uploaded_file = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± Ø³ÙØ§Ù„ÙˆÙ…ØªØ±ÛŒ", type=["png", "jpg", "jpeg"])

if uploaded_file:
    # Ø§ØµÙ„Ø§Ø­ Ø®Ø·Ø§ÛŒ Bytearray
    file_bytes = uploaded_file.read()
    image = Image.open(io.BytesIO(file_bytes)).convert("RGB")
    
    if 'landmarks' not in st.session_state:
        models = load_models()
        st.session_state.landmarks = get_predictions(image, models)

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Ù†Ù…Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ² Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ")
        
        # Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ± Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø®ØªØµØ§Øª Ø¨Ø§ Ú©Ù†ØªØ±Ù„ Ø®Ø·Ø§
        conf = {"x": 0, "y": 0}
        value = streamlit_image_coordinates(image, key="coords")
        
        if value is not None and isinstance(value, dict):
            # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ø±ÙˆÛŒ ØªØµÙˆÛŒØ± Ú©Ù„ÛŒÚ© Ú©Ø±Ø¯ØŒ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú© Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ú©Ù†
            target_idx = st.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© ÙØ¹Ø§Ù„:", range(29))
            st.session_state.landmarks[target_idx] = [value["x"], value["y"]]

    with col2:
        st.subheader("ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒÚ©Ø±ÙˆÙ…ØªØ±ÛŒ")
        idx = st.number_input("Ø§Ù†Ø¯ÛŒØ³ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© (0-28):", 0, 28, 0)
        col_x, col_y = st.columns(2)
        st.session_state.landmarks[idx][0] = col_x.number_input("X:", value=float(st.session_state.landmarks[idx][0]))
        st.session_state.landmarks[idx][1] = col_y.number_input("Y:", value=float(st.session_state.landmarks[idx][1]))

    # --- Clinical Calculations (Sample logic) ---
    st.divider()
    st.subheader("ğŸ“‘ Ú¯Ø²Ø§Ø±Ø´ Ú©Ù„ÛŒÙ†ÛŒÚ©ÛŒ Ù†Ù‡Ø§ÛŒÛŒ")
    
    # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ø§Øª (Ø¨Ø§ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø±ØªÙˆØ¯Ù†Ø³ÛŒ Ø´Ù…Ø§ ØªÚ©Ù…ÛŒÙ„ Ø´ÙˆØ¯)
    results = {
        "SNA Angle": "82.27Â°",
        "SNB Angle": "75.48Â°",
        "ANB Angle": "6.79Â°",
        "Skeletal Diagnosis": "Class II"
    }
    
    df_res = pd.DataFrame(list(results.items()), columns=["Ù¾Ø§Ø±Ø§Ù…ØªØ±", "Ù…Ù‚Ø¯Ø§Ø±"])
    st.table(df_res)

    if st.button("ğŸ“¥ Ø®Ø±ÙˆØ¬ÛŒ PDF"):
        st.success("Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
