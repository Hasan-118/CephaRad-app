import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import os
import cv2
from PIL import Image, ImageDraw, ImageFont
import torchvision.transforms as transforms
from streamlit_image_coordinates import streamlit_image_coordinates

# --- Û±. Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ UNet ---
class DoubleConv(nn.Module):
Â  Â  def __init__(self, in_ch, out_ch, dropout_prob=0.1):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.conv = nn.Sequential(
Â  Â  Â  Â  Â  Â  nn.Conv2d(in_ch, out_ch, 3, padding=1),
Â  Â  Â  Â  Â  Â  nn.BatchNorm2d(out_ch),
Â  Â  Â  Â  Â  Â  nn.ReLU(inplace=True),
Â  Â  Â  Â  Â  Â  nn.Dropout2d(p=dropout_prob),
Â  Â  Â  Â  Â  Â  nn.Conv2d(out_ch, out_ch, 3, padding=1),
Â  Â  Â  Â  Â  Â  nn.BatchNorm2d(out_ch),
Â  Â  Â  Â  Â  Â  nn.ReLU(inplace=True)
Â  Â  Â  Â  )
Â  Â  def forward(self, x): return self.conv(x)

class CephaUNet(nn.Module):
Â  Â  def __init__(self, n_landmarks=29):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.inc = DoubleConv(1, 64)
Â  Â  Â  Â  self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))
Â  Â  Â  Â  self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))
Â  Â  Â  Â  self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512, dropout_prob=0.3))
Â  Â  Â  Â  self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2)
Â  Â  Â  Â  self.conv_up1 = DoubleConv(512, 256, dropout_prob=0.3)
Â  Â  Â  Â  self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
Â  Â  Â  Â  self.conv_up2 = DoubleConv(256, 128)
Â  Â  Â  Â  self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)
Â  Â  Â  Â  self.conv_up3 = DoubleConv(128, 64)
Â  Â  Â  Â  self.outc = nn.Conv2d(64, n_landmarks, kernel_size=1)

Â  Â  def forward(self, x):
Â  Â  Â  Â  x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2); x4 = self.down3(x3)
Â  Â  Â  Â  x = self.up1(x4); x = torch.cat([x, x3], dim=1); x = self.conv_up1(x)
Â  Â  Â  Â  x = self.up2(x); x = torch.cat([x, x2], dim=1); x = self.conv_up2(x)
Â  Â  Â  Â  x = self.up3(x); x = torch.cat([x, x1], dim=1); x = self.conv_up3(x)
Â  Â  Â  Â  return self.outc(x)

# --- Û². Ù„ÙˆØ¯Ø± Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ CLAHE ---
@st.cache_resource
def load_aariz_engines():
Â  Â  paths = ['checkpoint_unet_clinical.pth', 'specialist_pure_model.pth', 'tmj_specialist_model.pth']
Â  Â  engines = []
Â  Â  for p in paths:
Â  Â  Â  Â  if os.path.exists(p):
Â  Â  Â  Â  Â  Â  model = CephaUNet(n_landmarks=29).to("cpu")
Â  Â  Â  Â  Â  Â  ckpt = torch.load(p, map_location="cpu")
Â  Â  Â  Â  Â  Â  state = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt
Â  Â  Â  Â  Â  Â  model.load_state_dict(state)
Â  Â  Â  Â  Â  Â  model.eval()
Â  Â  Â  Â  Â  Â  engines.append(model)
Â  Â  return engines

def get_ensemble_prediction(img_path, engines):
Â  Â  img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
Â  Â  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
Â  Â  img_enhanced = clahe.apply(img_gray)
Â  Â Â 
Â  Â  orig_h, orig_w = img_enhanced.shape
Â  Â  img_res = cv2.resize(img_enhanced, (384, 384), interpolation=cv2.INTER_LANCZOS4)
Â  Â  input_t = transforms.ToTensor()(img_res).unsqueeze(0)
Â  Â Â 
Â  Â  all_heatmaps = []
Â  Â  with torch.no_grad():
Â  Â  Â  Â  for model in engines:
Â  Â  Â  Â  Â  Â  all_heatmaps.append(model(input_t)[0].numpy())
Â  Â Â 
Â  Â  avg_output = np.mean(all_heatmaps, axis=0)
Â  Â  coords = {}
Â  Â  for i in range(29):
Â  Â  Â  Â  hm = avg_output[i]
Â  Â  Â  Â  y, x = np.unravel_index(np.argmax(hm), hm.shape)
Â  Â  Â  Â  coords[i] = [int(x * orig_w / 384), int(y * orig_h / 384)]
Â  Â  return coords, (orig_w, orig_h)

def get_angle(p1, p2, p3):
Â  Â  v1, v2 = np.array(p1)-np.array(p2), np.array(p3)-np.array(p2)
Â  Â  norm = np.linalg.norm(v1) * np.linalg.norm(v2)
Â  Â  return round(np.degrees(np.arccos(np.clip(np.dot(v1,v2)/norm, -1, 1))), 1) if norm != 0 else 0

# --- Û³. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø§ØµÙ„ÛŒ ---
st.set_page_config(layout="wide", page_title="Aariz Station v31.3")
landmark_names = ['A', 'ANS', 'B', 'Me', 'N', 'Or', 'Pog', 'PNS', 'Pn', 'R', 'S', 'Ar', 'Co', 'Gn', 'Go', 'Po', 'LPM', 'LIT', 'LMT', 'UPM', 'UIA', 'UIT', 'UMT', 'LIA', 'Li', 'Ls', 'N`', 'Pog`', 'Sn']
EXCELLENT_PTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 20, 21, 24, 25, 27, 28]
RELIABLE_PTS = [14, 17, 26]

engines = load_aariz_engines()

st.sidebar.title("ðŸ©º Aariz Control")
path_input = st.sidebar.text_input("Project Path:", value=os.getcwd())
img_dir = os.path.join(path_input, "Aariz", "train", "Cephalograms")

if os.path.exists(img_dir) and len(engines) > 0:
Â  Â  files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg'))])
Â  Â  selected_file = st.sidebar.selectbox("Choose Image:", files)
Â  Â  target_idx = st.sidebar.selectbox("Active Point:", range(29), format_func=lambda x: f"{x}: {landmark_names[x]}")
Â  Â Â 
Â  Â  full_path = os.path.join(img_dir, selected_file)
Â  Â  if "lms" not in st.session_state or st.session_state.get("file") != selected_file:
Â  Â  Â  Â  st.session_state.lms, st.session_state.orig_size = get_ensemble_prediction(full_path, engines)
Â  Â  Â  Â  st.session_state.file = selected_file

Â  Â  col_img, col_zoom = st.columns([2.5, 1])
Â  Â Â 
Â  Â  with col_img:
Â  Â  Â  Â  img_view = Image.open(full_path).convert("RGB")
Â  Â  Â  Â  orig_w, orig_h = st.session_state.orig_size
Â  Â  Â  Â  draw = ImageDraw.Draw(img_view)
Â  Â  Â  Â  l = st.session_state.lms

Â  Â  Â  Â  # Ø±Ø³Ù… Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ØªÙ† Ùˆ Ø§Ø³ØªØ±ÙˆÚ© Ø¨Ø²Ø±Ú¯ØªØ±
Â  Â  Â  Â  for i, pos in l.items():
Â  Â  Â  Â  Â  Â  is_active = (i == target_idx)
Â  Â  Â  Â  Â  Â  color = "#00FF00" if i in EXCELLENT_PTS else ("#FFFF00" if i in RELIABLE_PTS else "#FF00FF")
Â  Â  Â  Â  Â  Â  r = int(orig_w * 0.007)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Ø§ÙÚ©Øª Ù†Ù‚Ø·Ù‡ ÙØ¹Ø§Ù„
Â  Â  Â  Â  Â  Â  if is_active:
Â  Â  Â  Â  Â  Â  Â  Â  draw.ellipse([pos[0]-r-6, pos[1]-r-6, pos[0]+r+6, pos[1]+r+6], outline="red", width=6)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  draw.ellipse([pos[0]-r, pos[1]-r, pos[0]+r, pos[1]+r], fill=color, outline="white", width=2)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Ø¨Ø±Ú†Ø³Ø¨ Ù…ØªÙ†ÛŒ Ø¨Ø²Ø±Ú¯ Ùˆ Ø®ÙˆØ§Ù†Ø§
Â  Â  Â  Â  Â  Â  label_text = f"{i}:{landmark_names[i]}"
Â  Â  Â  Â  Â  Â  draw.text((pos[0]+r+5, pos[1]-r-15), label_text, fill="yellow", stroke_width=4, stroke_fill="black")

Â  Â  Â  Â  st.subheader("ðŸ“ Main Analysis View")
Â  Â  Â  Â  res = streamlit_image_coordinates(img_view, width=900, key="main_img")
Â  Â  Â  Â  if res:
Â  Â  Â  Â  Â  Â  scale = orig_w / 900
Â  Â  Â  Â  Â  Â  new_x, new_y = int(res["x"] * scale), int(res["y"] * scale)
Â  Â  Â  Â  Â  Â  if l[target_idx] != [new_x, new_y]:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.lms[target_idx] = [new_x, new_y]
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  with col_zoom:
Â  Â  Â  Â  st.subheader("ðŸ” Precision Zoom")
Â  Â  Â  Â  active_pos = st.session_state.lms[target_idx]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Ø¨Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒ Ø²ÙˆÙ…
Â  Â  Â  Â  z_size = 120
Â  Â  Â  Â  box = (max(0, active_pos[0]-z_size), max(0, active_pos[1]-z_size),Â 
Â  Â  Â  Â  Â  Â  Â  Â min(orig_w, active_pos[0]+z_size), min(orig_h, active_pos[1]+z_size))
Â  Â  Â  Â Â 
Â  Â  Â  Â  zoom_img = Image.open(full_path).convert("RGB").crop(box)
Â  Â  Â  Â  z_draw = ImageDraw.Draw(zoom_img)
Â  Â  Â  Â  # Ø±Ø³Ù… Crosshair Ø¯Ø± Ù…Ø±Ú©Ø² Ø²ÙˆÙ…
Â  Â  Â  Â  cw, ch = zoom_img.size
Â  Â  Â  Â  z_draw.line([(cw//2, 0), (cw//2, ch)], fill="red", width=2)
Â  Â  Â  Â  z_draw.line([(0, ch//2), (cw, ch//2)], fill="red", width=2)
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.image(zoom_img, use_container_width=True, caption=f"Centering: {landmark_names[target_idx]}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.write("### âŒ¨ï¸ Micro-Movements")
Â  Â  Â  Â  c1, c2, c3 = st.columns(3)
Â  Â  Â  Â  if c2.button("ðŸ”¼"): st.session_state.lms[target_idx][1] -= 1; st.rerun()
Â  Â  Â  Â  if c1.button("â—€ï¸"): st.session_state.lms[target_idx][0] -= 1; st.rerun()
Â  Â  Â  Â  if c3.button("â–¶ï¸"): st.session_state.lms[target_idx][0] += 1; st.rerun()
Â  Â  Â  Â  if c2.button("ðŸ”½"): st.session_state.lms[target_idx][1] += 1; st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  if st.button("ðŸ”„ Reset Point", use_container_width=True):
Â  Â  Â  Â  Â  Â  fresh, _ = get_ensemble_prediction(full_path, engines)
Â  Â  Â  Â  Â  Â  st.session_state.lms[target_idx] = fresh[target_idx]
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("ðŸ’¾ Save & Lock", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  st.success("Landmark Coordinate Locked!")

Â  Â  # Ø¢Ù†Ø§Ù„ÛŒØ² Ù¾Ø§ÛŒÛŒÙ†
Â  Â  st.divider()
Â  Â  sna = get_angle(l[10], l[4], l[0])
Â  Â  snb = get_angle(l[10], l[4], l[2])
Â  Â  anb = round(sna - snb, 1)
Â  Â Â 
Â  Â Â 
Â  Â Â 
Â  Â  col1, col2, col3 = st.columns(3)
Â  Â  col1.metric("SNA", f"{sna}Â°")
Â  Â  col2.metric("SNB", f"{snb}Â°")
Â  Â  col3.metric("ANB", f"{anb}Â°", delta="Class II" if anb > 4 else ("Class III" if anb < 0 else "Class I"))

else:
Â  Â  st.error("Missing .pth files or images directory!")
