import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import os
import gdown
from PIL import Image, ImageDraw

# --- Û±. Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¯Ù‚ÛŒÙ‚ UNet (ØªØ·Ø¨ÛŒÙ‚ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø´Ù…Ø§) ---
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.double_conv(x)

class Down(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))
    def forward(self, x): return self.maxpool_conv(x)

class Up(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=29, bilinear=True):
        super(UNet, self).__init__()
        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        factor = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        self.up1 = Up(1024, 512 // factor, bilinear)
        self.up2 = Up(512, 256 // factor, bilinear)
        self.up3 = Up(256, 128 // factor, bilinear)
        self.up4 = Up(128, 64, bilinear)
        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        return self.outc(x)

# --- Û². Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªØ±Ú©ÛŒØ¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Ensemble Logic) ---
@st.cache_resource
def load_ensemble():
    device = torch.device('cpu')
    drive_ids = {
        'general': '1a1sZ2z0X6mOwljhBjmItu_qrWYv3v_ks', 
        'specialist': '1RakXVfUC_ETEdKGBi6B7xOD7MjD59jfU', 
        'tmj': '1tizRbUwf7LgC6Radaeiz6eUffiwal0cH'
    }
    os.makedirs('models', exist_ok=True)
    models = []
    for name, fid in drive_ids.items():
        path = f"models/{name}.pth"
        if not os.path.exists(path):
            gdown.download(id=fid, output=path, quiet=False)
        
        m = UNet(n_channels=1, n_classes=29)
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ (Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ bilinear Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ True Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª)
        state_dict = torch.load(path, map_location=device)
        m.load_state_dict(state_dict, strict=False)
        m.eval()
        models.append(m)
    return models

# --- Û³. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø®ØªØµØ§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø§Ú©Ø²ÛŒÙ…Ù… Ù‡ÛŒØªâ€ŒÙ…Ù¾ ---
def get_landmarks(models, image_tensor, original_size):
    with torch.no_grad():
        # Ensemble: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Logits) Ùˆ Ø³Ù¾Ø³ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
        all_outputs = [m(image_tensor) for m in models]
        avg_output = torch.mean(torch.stack(all_outputs), dim=0)
        
        # Ø§Ø¹Ù…Ø§Ù„ Sigmoid Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÛŒØªâ€ŒÙ…Ù¾â€ŒÙ‡Ø§
        avg_output = torch.sigmoid(avg_output)
    
    w_orig, h_orig = original_size
    # Ù…Ù‡Ù…: Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛµÛ±Û² Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ØŒ Ù¾Ø³ Ø¨Ø§ÛŒØ¯ Ù…Ø®ØªØµØ§Øª Ø±Ø§ Ø¨Ù‡ Ø³Ø§ÛŒØ² Ø§ØµÙ„ÛŒ Ù…Ù¾ Ú©Ù†ÛŒÙ…
    scale_x, scale_y = w_orig / 512, h_orig / 512
    
    landmarks = []
    output_np = avg_output[0].cpu().numpy() # [29, 512, 512]
    
    for i in range(29):
        heatmap = output_np[i]
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù¾ÛŒÚ©Ø³Ù„ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± (Ù†Ù‚Ø·Ù‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©)
        _, _, _, max_loc = cv2.minMaxLoc(heatmap)
        # ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ø¨Ù‡ Ø³Ø§ÛŒØ² ÙˆØ§Ù‚Ø¹ÛŒ ØªØµÙˆÛŒØ±
        landmarks.append((int(max_loc[0] * scale_x), int(max_loc[1] * scale_y)))
    
    return landmarks

# --- Û´. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (Streamlit) ---
st.title("ğŸ¦· Ø¢Ù†Ø§Ù„ÛŒØ² Ø¯Ù‚ÛŒÙ‚ CephRad Ensemble")

uploaded_file = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± Ø³ÙØ§Ù„ÙˆÙ…ØªØ±ÛŒ", type=['png', 'jpg', 'jpeg'])

if uploaded_file:
    img = Image.open(uploaded_file).convert('RGB')
    w, h = img.size
    
    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ù‚ÛŒÙ‚ Ù…Ø·Ø§Ø¨Ù‚ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©
    img_gray = img.convert('L').resize((512, 512))
    img_tensor = torch.from_numpy(np.array(img_gray)).float().unsqueeze(0).unsqueeze(0) / 255.0
    
    if st.button("ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†Ø³Ø§Ù…Ø¨Ù„ Ùˆ Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ"):
        models = load_ensemble()
        pts = get_landmarks(models, img_tensor, (w, h))
        
        # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· Ø±ÙˆÛŒ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
        draw = ImageDraw.Draw(img)
        for i, (px, py) in enumerate(pts):
            # Ø¯Ø§ÛŒØ±Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
            draw.ellipse([px-10, py-10, px+10, py+10], fill='red', outline='white')
            # Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú© Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† ØªØ±ØªÛŒØ¨
            draw.text((px+12, py-12), str(i+1), fill='yellow')
            
        st.image(img, caption="Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ (Ensemble)", use_column_width=True)
